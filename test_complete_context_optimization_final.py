#!/usr/bin/env python3
"""
üèóÔ∏è COMPLETE CONTEXT OPTIMIZATION FINAL TEST
Comprehensive test of all context optimization features
"""

import sys
import os
import asyncio
import time
sys.path.append('./backend')

async def test_complete_system():
    """Test complete context optimization system"""
    print("üèóÔ∏è COMPLETE CONTEXT OPTIMIZATION FINAL TEST")
    print("=" * 80)
    
    try:
        from core.run import ToolManager
        from core.agentpress.thread_manager import ThreadManager
        from core.agentpress.context_manager import ContextManager
        
        # Initialize all components
        thread_manager = ThreadManager()
        tool_manager = ToolManager(thread_manager, project_id="test-project", thread_id="test-thread")
        context_manager = ContextManager()
        
        print("üìä SYSTEM INITIALIZATION")
        print("-" * 40)
        
        # Register tools
        start_time = time.time()
        tool_manager.register_all_tools()
        registration_time = time.time() - start_time
        
        total_tools = len(thread_manager.tool_registry.tools)
        print(f"‚úÖ Tools registered: {total_tools} in {registration_time:.3f}s")
        
        # Test scenarios covering all functionality
        test_scenarios = [
            {
                "name": "Web Research & Task Creation",
                "query": "Search for Python optimization best practices and create a task list",
                "expected_tools": ["web_search", "create_tasks", "update_tasks", "ask", "complete"],
                "expected_reduction": (50, 70),
                "category": "Mixed Operations"
            },
            {
                "name": "File Operations & Code Editing",
                "query": "Edit Python files and create new modules for optimization",
                "expected_tools": ["str_replace", "create_file", "edit_file", "search_files"],
                "expected_reduction": (50, 70),
                "category": "Development"
            },
            {
                "name": "Data Analysis & Spreadsheets",
                "query": "Analyze spreadsheet data and create visualizations",
                "expected_tools": ["analyze_sheet", "create_sheet", "view_sheet"],
                "expected_reduction": (45, 60),
                "category": "Data Operations"
            },
            {
                "name": "Command Execution & Browser",
                "query": "Execute commands and navigate websites for testing",
                "expected_tools": ["execute_command", "browser_navigate_to", "browser_act"],
                "expected_reduction": (55, 75),
                "category": "System Operations"
            },
            {
                "name": "Presentation & Documentation",
                "query": "Create presentation slides and documentation",
                "expected_tools": ["create_slide", "create_document", "present_presentation"],
                "expected_reduction": (55, 75),
                "category": "Content Creation"
            }
        ]
        
        print(f"\nüìä CONTEXT OPTIMIZATION TESTING")
        print("-" * 40)
        
        all_passed = True
        results = []
        
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\nüéØ Scenario {i}: {scenario['name']}")
            print(f"   Category: {scenario['category']}")
            print(f"   Query: '{scenario['query'][:50]}...'")
            
            # Test tool filtering
            start_time = time.time()
            filtered_schemas = thread_manager.tool_registry.get_filtered_schemas(scenario['query'])
            filtering_time = time.time() - start_time
            
            filtered_count = len(filtered_schemas)
            reduction_ratio = (1 - filtered_count/total_tools) * 100
            
            # Extract tool names
            tool_names = []
            for schema in filtered_schemas:
                if 'function' in schema and 'name' in schema['function']:
                    tool_names.append(schema['function']['name'])
            
            print(f"   Tools filtered: {filtered_count}/{total_tools} ({reduction_ratio:.1f}% reduction)")
            print(f"   Filtering time: {filtering_time:.3f}s")
            
            # Check expected tools
            expected_found = 0
            missing_tools = []
            for expected_tool in scenario['expected_tools']:
                if expected_tool in tool_names:
                    expected_found += 1
                else:
                    missing_tools.append(expected_tool)
            
            print(f"   Expected tools found: {expected_found}/{len(scenario['expected_tools'])}")
            
            # Check essential tools
            essential_tools = ['ask', 'complete', 'web_search', 'create_tasks']
            essential_found = sum(1 for tool in essential_tools if tool in tool_names)
            print(f"   Essential tools: {essential_found}/{len(essential_tools)}")
            
            # Validate reduction ratio
            min_reduction, max_reduction = scenario['expected_reduction']
            reduction_ok = min_reduction <= reduction_ratio <= max_reduction
            
            # Test system prompt optimization
            if hasattr(context_manager, 'get_optimized_system_prompt'):
                large_prompt = """You are Augment Agent developed by Augment Code, an agentic coding AI assistant with access to the developer's codebase through Augment's world-leading context engine and integrations.
                
                # Identity and Capabilities
                The base model is Claude Sonnet 4 by Anthropic with comprehensive tool access.
                
                # Guidelines and Instructions
                - Search for information before making edits
                - Use task management tools for complex work
                - Always use package managers for dependencies
                - Focus on following user instructions precisely
                - Use parallel tool calls for efficiency
                """ * 3
                
                optimized_prompt = context_manager.get_optimized_system_prompt(scenario['query'], large_prompt)
                prompt_reduction = ((len(large_prompt) - len(optimized_prompt)) / len(large_prompt)) * 100
                print(f"   System prompt reduction: {prompt_reduction:.1f}%")
            else:
                prompt_reduction = 0
                print(f"   System prompt optimization: Not available")
            
            # Scenario assessment
            scenario_passed = (
                expected_found >= len(scenario['expected_tools']) * 0.8 and  # 80% of expected tools
                essential_found >= 3 and  # Most essential tools
                reduction_ok and  # Reduction within range
                filtering_time < 0.1  # Fast filtering
            )
            
            if scenario_passed:
                print(f"   ‚úÖ PASS")
            else:
                print(f"   ‚ùå FAIL")
                if missing_tools:
                    print(f"      Missing tools: {missing_tools}")
                if not reduction_ok:
                    print(f"      Reduction outside range: {reduction_ratio:.1f}% not in {min_reduction}-{max_reduction}%")
                if filtering_time >= 0.1:
                    print(f"      Filtering too slow: {filtering_time:.3f}s >= 0.1s")
                all_passed = False
            
            results.append({
                'scenario': scenario['name'],
                'passed': scenario_passed,
                'tools_filtered': filtered_count,
                'reduction_ratio': reduction_ratio,
                'filtering_time': filtering_time,
                'expected_found': expected_found,
                'essential_found': essential_found,
                'prompt_reduction': prompt_reduction
            })
        
        # Overall assessment
        print(f"\nüìä OVERALL ASSESSMENT")
        print("=" * 40)
        
        passed_scenarios = sum(1 for r in results if r['passed'])
        total_scenarios = len(results)
        success_rate = (passed_scenarios / total_scenarios) * 100
        
        avg_reduction = sum(r['reduction_ratio'] for r in results) / len(results)
        avg_filtering_time = sum(r['filtering_time'] for r in results) / len(results)
        avg_prompt_reduction = sum(r['prompt_reduction'] for r in results) / len(results)
        
        print(f"Success Rate: {success_rate:.1f}% ({passed_scenarios}/{total_scenarios})")
        print(f"Average Tool Reduction: {avg_reduction:.1f}%")
        print(f"Average Filtering Time: {avg_filtering_time:.3f}s")
        print(f"Average Prompt Reduction: {avg_prompt_reduction:.1f}%")
        print(f"Tool Registration Time: {registration_time:.3f}s")
        
        # Performance benchmarks
        print(f"\nüéØ PERFORMANCE BENCHMARKS")
        print("-" * 40)
        
        benchmarks = {
            "Tool Registration": (registration_time < 5.0, f"{registration_time:.3f}s < 5.0s"),
            "Average Filtering": (avg_filtering_time < 0.1, f"{avg_filtering_time:.3f}s < 0.1s"),
            "Tool Reduction": (50 <= avg_reduction <= 70, f"{avg_reduction:.1f}% in 50-70% range"),
            "Prompt Reduction": (avg_prompt_reduction >= 70, f"{avg_prompt_reduction:.1f}% >= 70%"),
            "Success Rate": (success_rate >= 80, f"{success_rate:.1f}% >= 80%")
        }
        
        all_benchmarks_passed = True
        for benchmark, (passed, description) in benchmarks.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"{benchmark}: {status} ({description})")
            if not passed:
                all_benchmarks_passed = False
        
        # Final verdict
        print(f"\nüèÅ FINAL VERDICT")
        print("=" * 40)
        
        if all_passed and all_benchmarks_passed and success_rate >= 90:
            print(f"üéâ EXCELLENT: Context optimization working optimally!")
            print(f"   ‚úÖ All scenarios passed")
            print(f"   ‚úÖ All benchmarks met")
            print(f"   ‚úÖ Production ready")
            return True
        elif success_rate >= 80:
            print(f"‚úÖ GOOD: Context optimization working well")
            print(f"   ‚ö†Ô∏è Some minor issues may need attention")
            return True
        else:
            print(f"‚ö†Ô∏è NEEDS WORK: Context optimization has issues")
            print(f"   ‚ùå Review failed scenarios and benchmarks")
            return False
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Main test function"""
    print("üöÄ STARTING COMPLETE CONTEXT OPTIMIZATION FINAL TEST")
    print("=" * 80)
    print("Testing all aspects of context optimization implementation")
    print("Including tool filtering, system prompt optimization, and performance")
    print("=" * 80)
    
    success = await test_complete_system()
    
    print(f"\nüèÅ FINAL RESULT:")
    if success:
        print(f"üéâ CONTEXT OPTIMIZATION SYSTEM: EXCELLENT")
        print(f"   ‚úÖ All functionality working optimally")
        print(f"   ‚úÖ Performance targets achieved")
        print(f"   ‚úÖ Ready for production deployment")
    else:
        print(f"‚ö†Ô∏è CONTEXT OPTIMIZATION SYSTEM: NEEDS ATTENTION")
        print(f"   ‚ùå Some issues detected")
        print(f"   ‚ö†Ô∏è Review test results and fix issues")

if __name__ == "__main__":
    asyncio.run(main())
