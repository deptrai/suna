#!/usr/bin/env python3
"""
Debug full flow from auto selection to LiteLLM call.
"""

import os
import sys
import asyncio
sys.path.append('backend')

# Set environment variables
os.environ['AUTO_MODEL_ENABLED'] = 'true'
os.environ['ENV_MODE'] = 'local'

from backend.core.ai_models.manager import model_manager
from backend.core.services.llm import make_llm_api_call, prepare_params
from backend.core.utils.config import config

async def debug_full_flow():
    """Debug the complete flow."""
    
    print("ğŸ” DEBUGGING FULL FLOW")
    print("=" * 80)
    
    # Step 1: Test auto selection
    print("\nğŸ“ STEP 1: Auto Selection")
    query = "Test with Router strategy for v98store"
    
    try:
        selected_model = model_manager.resolve_model_id("auto", query)
        print(f"âœ… Auto selected model: {selected_model}")
    except Exception as e:
        print(f"âŒ Auto selection failed: {e}")
        return
    
    # Step 2: Test prepare_params
    print("\nğŸ“ STEP 2: Prepare Params")
    
    try:
        messages = [{"role": "user", "content": query}]
        params = prepare_params(
            messages=messages,
            model_name=selected_model,
            temperature=0.7,
            max_tokens=100,
            stream=False
        )
        
        print(f"âœ… Prepared params:")
        for key, value in params.items():
            if key == 'api_key':
                print(f"   {key}: {value[:10]}...{value[-4:] if value else 'None'}")
            elif key == 'messages':
                print(f"   {key}: [message with {len(value)} items]")
            else:
                print(f"   {key}: {value}")
                
    except Exception as e:
        print(f"âŒ Prepare params failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Step 3: Test strategy selection
    print("\nğŸ“ STEP 3: Strategy Selection")
    
    try:
        from backend.core.ai_models.llm_strategies import LLMCallContext
        from backend.core.services.llm import provider_router
        
        call_context = LLMCallContext(selected_model, provider_router)
        strategy_info = call_context.get_strategy_info()
        print(f"âœ… Strategy: {strategy_info}")
        
    except Exception as e:
        print(f"âŒ Strategy selection failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Step 4: Test router configuration
    print("\nğŸ“ STEP 4: Router Configuration")
    
    try:
        from backend.core.services.llm import provider_router
        
        if provider_router:
            print(f"âœ… Router exists: {type(provider_router)}")
            
            # Check router model list
            if hasattr(provider_router, 'model_list'):
                print(f"âœ… Router model list:")
                for i, model_config in enumerate(provider_router.model_list):
                    print(f"   Model {i+1}: {model_config}")
            else:
                print("âŒ Router has no model_list")
        else:
            print("âŒ Router is None")
            
    except Exception as e:
        print(f"âŒ Router check failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Step 5: Test actual LLM call
    print("\nğŸ“ STEP 5: Actual LLM Call")
    
    try:
        print(f"ğŸš€ Making LLM call with model: {selected_model}")
        
        response = await make_llm_api_call(
            messages=messages,
            model_name=selected_model,
            temperature=0.7,
            max_tokens=100,
            stream=False
        )
        
        print(f"âœ… LLM call successful!")
        if hasattr(response, 'choices') and response.choices:
            content = response.choices[0].message.content
            print(f"ğŸ“ Response: {content[:100]}...")
        else:
            print(f"ğŸ“ Response type: {type(response)}")
            
    except Exception as e:
        print(f"âŒ LLM call failed: {e}")
        import traceback
        traceback.print_exc()
    
    # Step 6: Test direct router call
    print("\nğŸ“ STEP 6: Direct Router Call")
    
    try:
        from backend.core.services.llm import provider_router
        
        if provider_router:
            print(f"ğŸš€ Testing direct router call...")
            
            # Test with original model name
            test_params = {
                "model": selected_model,  # Use original model name
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 100
            }
            
            print(f"ğŸ“ Router test params: {test_params}")
            
            response = await provider_router.acompletion(**test_params)
            
            print(f"âœ… Direct router call successful!")
            if hasattr(response, 'choices') and response.choices:
                content = response.choices[0].message.content
                print(f"ğŸ“ Response: {content[:100]}...")
                
        else:
            print("âŒ Router is None, cannot test direct call")
            
    except Exception as e:
        print(f"âŒ Direct router call failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ Full Flow Debug")
    print("=" * 60)
    
    asyncio.run(debug_full_flow())
    
    print("\nâœ… Debug completed!")
