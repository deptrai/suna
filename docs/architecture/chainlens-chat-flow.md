# ChainLens Chat Flow Diagram

Biá»ƒu Ä‘á»“ luá»“ng xá»­ lÃ½ request-to-response cá»§a há»‡ thá»‘ng ChainLens tá»« frontend Ä‘áº¿n backend.

## Overview

SÆ¡ Ä‘á»“ nÃ y mÃ´ táº£ chi tiáº¿t quÃ¡ trÃ¬nh xá»­ lÃ½ tá»« khi user gá»­i message Ä‘áº¿n khi nháº­n Ä‘Æ°á»£c response, bao gá»“m:
- Frontend UI components vÃ  model selection
- Backend API routing vÃ  agent run management
- **ğŸ¤– Auto Model Selection** vá»›i intelligent query analysis
- **ğŸ§  Context Optimization** vá»›i multi-stage compression
- **ğŸ¢ v98store Integration** vá»›i 9 premium models
- **ğŸ”§ Smart Tool Management** vá»›i provider compatibility
- Tool execution vÃ  response processing
- Error handling vÃ  fallback mechanisms

---

```mermaid
flowchart TD
    %% User starts conversation
    START(["ğŸ‘¤ User starts chat"]) --> UI

    %% Frontend Layer
    subgraph FE ["ğŸŒ Frontend Layer"]
        UI["ğŸ“± User Interface<br/>ThreadComponent.tsx"]
        CI["âœï¸ Chat Input<br/>chat-input.tsx"]
        MS["âš™ï¸ Model Selection<br/>use-model-selection.ts"]
        UI --> CI
        CI --> MS
    end

    %% API Gateway
    subgraph API ["ğŸ”— FastAPI Backend"]
        MAIN["âš¡ Main API<br/>api.py"]
        CORE["ğŸ¯ Core Router<br/>core/api_main.py"]
        RUNS["ğŸƒ Agent Runs<br/>core/agent_runs.py"]
        MAIN --> CORE
        CORE --> RUNS
    end

    %% Background Processing
    subgraph BG ["âš™ï¸ Background Processing"]
        DRAMA["ğŸ­ Dramatiq Worker<br/>run_agent_background.py"]
        REDIS["ğŸ“¦ Redis State<br/>Locks & Pub/Sub"]
        CORE_RUN["ğŸ§  Core Agent<br/>core/run.py"]
        DRAMA <--> REDIS
        DRAMA --> CORE_RUN
    end

    %% Model Management with Auto Detection
    AUTO{"ï¿½ Auto Model?<br/>model == 'auto'"}
    HEURISTIC["ğŸ¯ Smart Analysis<br/>Query complexity<br/>Keyword detection<br/>Length analysis"]
    RESOLVE["âš™ï¸ Model Resolution<br/>model_manager"]

    %% LLM Service Layer
    subgraph LLM ["ğŸ§  LLM Service"]
        PREP["ğŸ”§ Prepare Params<br/>prepare_params()"]
        ROUTER["ğŸ”„ LiteLLM Router<br/>acompletion()"]
        PROVIDERS["ğŸ¢ Providers<br/>OpenAI | Anthropic<br/>xAI | Groq | v98store"]
        TOOL_FILTER["ğŸ”§ Tool Filter<br/>Provider compatibility"]
        PREP --> TOOL_FILTER
        TOOL_FILTER --> ROUTER
        ROUTER --> PROVIDERS
    end

    %% Tool System
    subgraph TOOLS ["ğŸ”§ Tool System"]
        TM["ğŸ› ï¸ Tool Manager"]
        TR["ğŸ“‹ Tool Registry"]
        EXEC["âš¡ Tool Execution"]
        TM --> TR
        TR --> EXEC
    end

    %% Response Processing
    STREAM{"ğŸŒŠ Streaming?"}
    PROC_S["ğŸ“¡ Stream Processor"]
    PROC_NS["ğŸ“„ Batch Processor"]
    PERSIST["ğŸ’¾ Database<br/>Supabase"]

    %% Error Handling
    subgraph ERROR ["ğŸš¨ Error Handling"]
        FALLBACK["ğŸ”„ Fallback<br/>OpenRouter"]
        RETRY["ğŸ” Retry Logic"]
        ERR_LOG["ğŸ“ Error Logs"]
        FALLBACK --> RETRY
        RETRY --> ERR_LOG
    end

    %% Context Management
    subgraph CONTEXT ["ğŸ§  Context Optimization"]
        TOKEN["ğŸ”¢ Token Count<br/>litellm.token_counter"]
        COMPRESS["ğŸ“¦ Multi-stage Compression<br/>Tool results | User msgs | Assistant msgs"]
        LIMIT["ğŸ“ Message Limiting<br/>Recent messages priority"]
        CACHE["ğŸ’¾ Caching<br/>Anthropic cache control"]
        TOKEN --> COMPRESS
        COMPRESS --> LIMIT
        LIMIT --> CACHE
    end

    %% Main Flow Connections
    MS --> MAIN
    RUNS --> DRAMA
    CORE_RUN --> AUTO
    AUTO -->|Yes| HEURISTIC
    AUTO -->|No| RESOLVE
    HEURISTIC --> RESOLVE
    RESOLVE --> PREP
    
    %% LLM to Tools (conditional based on provider compatibility)
    PROVIDERS --> TOOLS
    EXEC --> STREAM
    
    %% Response Processing Flow
    STREAM -->|Yes| PROC_S
    STREAM -->|No| PROC_NS
    PROC_S --> PERSIST
    PROC_NS --> PERSIST
    
    %% Context Management Flow
    CORE_RUN --> CONTEXT
    CONTEXT --> PREP
    
    %% Error Handling Flow
    PROVIDERS -.->|"âŒ Error"| ERROR
    ERROR -.->|"ğŸ”„ Retry"| PROVIDERS
    
    %% Final Response
    PERSIST --> UI
    
    %% End point
    UI --> FINISH(["âœ… Response delivered"])

    %% Styling
    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef backend fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef processing fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef llm fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef tools fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#fff9c4,stroke:#f9a825,stroke-width:3px
    classDef error fill:#ffebee,stroke:#d32f2f,stroke-width:2px
    classDef startend fill:#e0e0e0,stroke:#424242,stroke-width:2px

    class UI,CI,MS frontend
    class MAIN,CORE,RUNS backend
    class DRAMA,REDIS,CORE_RUN processing
    class PREP,ROUTER,PROVIDERS llm
    class TM,TR,EXEC tools
    class AUTO,HEURISTIC,STREAM decision
    class FALLBACK,RETRY,ERR_LOG error
    class START,FINISH startend
```

---

## Key Components Explanation

### ğŸŒ **Frontend Layer**
- **ThreadComponent.tsx**: Main chat interface quáº£n lÃ½ conversation state
- **chat-input.tsx**: Input component xá»­ lÃ½ user message submission
- **use-model-selection.ts**: Hook quáº£n lÃ½ model selection vá»›i "auto" option vÃ  v98store models

### ğŸ”— **FastAPI Backend**
- **api.py**: Main application vá»›i CORS, middleware setup
- **core/api_main.py**: Core routing vá»›i sub-routers cho various services
- **core/agent_runs.py**: Agent lifecycle management (start/stop/status)

### âš™ï¸ **Background Processing**
- **Dramatiq Worker**: Background task processing vá»›i Redis coordination
- **Redis State Management**: Locks, TTLs, pub/sub cho distributed coordination
- **Core Agent Run**: Main execution logic trong `core/run.py`

### ğŸ¤– **Auto Model Selection** *(NEW)*
- **Intelligent Query Analysis**: Keyword detection cho complex tasks (code, implement, analyze, etc.)
- **Length-based Detection**: Queries > 8 words Ä‘Æ°á»£c classify as complex
- **Smart Model Mapping**:
  - Complex queries â†’ `openai-compatible/gpt-4o` (premium model)
  - Simple queries â†’ `openai-compatible/gpt-4o-mini` (efficient model)
- **Environment Control**: `AUTO_MODEL_ENABLED=true` Ä‘á»ƒ enable feature
- **Model Resolution**: `model_manager.resolve_model_id()` vá»›i auto selection logic

### ğŸ§  **LLM Service Layer**
- **Parameter Preparation**: Provider-specific configurations
- **Tool Schema Filtering**: Smart detection cho provider compatibility
- **LiteLLM Router**: Unified interface cho multiple providers
- **Provider Support**: OpenAI, Anthropic, xAI, Groq, OpenRouter, Bedrock, **v98store**

### ğŸ¢ **v98store Integration** *(NEW)*
- **9 Premium Models**: GPT-4o, GPT-5, Qwen 3 32B/235B, Claude 3.7 Sonnet, Grok 4, Kimi K2
- **OpenAI-Compatible API**: Seamless integration thÃ´ng qua LiteLLM Router
- **Tool Schema Compatibility**: Automatic tool filtering cho v98store models
- **Model Aliases**: Support multiple naming conventions (gpt-4o, v98store/gpt-4o, etc.)
- **Cost Optimization**: Premium models accessible vá»›i competitive pricing

### ğŸ”§ **Smart Tool Management** *(ENHANCED)*
- **Tool Manager**: Register various agent capabilities
- **Tool Registry**: OpenAPI schemas cho native tool calling
- **Provider Compatibility Detection**: Automatic tool filtering based on model provider
- **v98store Compatibility**: Tools disabled cho v98store models (khÃ´ng support tool schemas)
- **Tool Execution**: Function calls vá»›i error handling

### ğŸ“¤ **Response Processing**
- **Streaming Support**: Real-time response chunks
- **Non-Streaming**: Complete response processing
- **Database Persistence**: Supabase storage cho conversation history

### ğŸš¨ **Error Handling**
- **Fallback Mechanisms**: OpenRouter backup khi primary provider fails
- **Retry Logic**: Exponential backoff cho transient errors
- **Comprehensive Logging**: Structured logs vá»›i Langfuse tracing

### ğŸ§  **Context Optimization** *(ENHANCED)*
- **Accurate Token Counting**: `litellm.token_counter` cho precise measurements
- **Multi-stage Compression**:
  - Tool result messages compression (except most recent)
  - User message compression (except most recent)
  - Assistant message compression (except most recent)
- **Message Limiting**: Keep recent messages vá»›i priority system
- **Context Window Utilization (CWU)**: Monitor vÃ  optimize usage (target 60-70%)
- **Balanced Threshold**: 25,000 tokens cho better tool availability
- **Caching**: Anthropic cache control cho cost optimization

---

## Flow Descriptions

### ğŸ¯ **Main Request Flow**
1. User inputs message trong chat interface
2. Model selection (auto/specific) Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh
3. FastAPI routes request qua core API
4. Agent run Ä‘Æ°á»£c táº¡o vÃ  queued trong background
5. Dramatiq worker picks up task
6. Smart model routing analyzes vÃ  selects optimal model
7. LLM API call Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i appropriate parameters
8. Response Ä‘Æ°á»£c processed (streaming/non-streaming)
9. Tools Ä‘Æ°á»£c executed náº¿u needed
10. Results Ä‘Æ°á»£c persisted vÃ  returned to frontend

### ğŸ¤– **Auto Model Selection Flow** *(ENHANCED)*
1. Check if model === "auto" vÃ  `AUTO_MODEL_ENABLED=true`
2. **Query Analysis**:
   - Keyword detection: ['code', 'implement', 'create', 'analyze', 'design', 'strategy', 'build', 'develop', 'function', 'class', 'algorithm', 'optimize', 'debug', 'refactor']
   - Length analysis: > 8 words = complex
3. **Model Selection Logic**:
   - Complex queries â†’ `openai-compatible/gpt-4o` (premium v98store model)
   - Simple queries â†’ `openai-compatible/gpt-4o-mini` (efficient v98store model)
4. Resolve model ID thÃ´ng qua model manager
5. Use resolved model cho LLM API call

### ğŸš¨ **Error Handling Flow**
1. Primary provider fails
2. Fallback to OpenRouter equivalent
3. Retry with exponential backoff
4. Log structured error information
5. Return user-friendly error message náº¿u all attempts fail

---

### ğŸ§  **Context Optimization Flow** *(NEW)*
1. **Token Counting**: Use `litellm.token_counter` cho accurate measurement
2. **Message Limiting**: Keep recent messages (max 8) + system message
3. **Multi-stage Compression**:
   - Compress tool results (except most recent)
   - Compress user messages (except most recent)
   - Compress assistant messages (except most recent)
4. **CWU Monitoring**: Calculate Context Window Utilization ratio
5. **Iterative Optimization**: Repeat until under threshold hoáº·c max iterations

## Technical Notes

- **Redis Keys**: `active_run:{instance_id}:{agent_run_id}` format
- **Control Channels**: `agent_run:{agent_run_id}:control` cho stop signals
- **Response Lists**: `agent_run:{agent_run_id}:responses` cho streaming data
- **Tool Calling**: Native support thÃ´ng qua OpenAPI schemas vá»›i provider compatibility
- **Context Limits**: Automatic compression khi token threshold exceeded (25,000 tokens)
- **Provider Fallbacks**: Mapped cho high availability
- **v98store API**: `https://v98store.com/v1` vá»›i OpenAI-compatible interface
- **Tool Schema Filtering**: Automatic detection vÃ  skip cho incompatible providers
- **Auto Model Environment**: `AUTO_MODEL_ENABLED=true` Ä‘á»ƒ enable intelligent selection

---

## Recent Updates (v3.1)

### âœ¨ **New Features Added:**
- **ğŸ¤– Auto Model Selection**: Intelligent model selection based on query complexity
- **ğŸ§  Context Optimization**: Multi-stage compression vá»›i CWU monitoring
- **ğŸ¢ v98store Integration**: 9 premium models vá»›i competitive pricing
- **ğŸ”§ Smart Tool Management**: Provider compatibility detection

### ğŸ”§ **Technical Improvements:**
- **Token Counting**: Accurate measurement vá»›i `litellm.token_counter`
- **Message Compression**: Multi-stage approach cho optimal context usage
- **Tool Schema Filtering**: Automatic compatibility detection
- **Model Registry**: Enhanced vá»›i v98store models vÃ  aliases

---

*Diagram updated: 2025-09-28*
*Source: ChainLens codebase analysis - Version 3.1*