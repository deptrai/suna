# ChainLens Chat Flow Diagram

Biá»ƒu Ä‘á»“ luá»“ng xá»­ lÃ½ request-to-response cá»§a há»‡ thá»‘ng ChainLens tá»« frontend Ä‘áº¿n backend.

## Overview

SÆ¡ Ä‘á»“ nÃ y mÃ´ táº£ chi tiáº¿t quÃ¡ trÃ¬nh xá»­ lÃ½ tá»« khi user gá»­i message Ä‘áº¿n khi nháº­n Ä‘Æ°á»£c response, bao gá»“m:
- Frontend UI components vÃ  model selection
- Backend API routing vÃ  agent run management
- **ğŸ¤– Auto Model Selection** vá»›i intelligent query analysis
- **ğŸ§  Context Optimization** vá»›i multi-stage compression
- **ğŸ¢ v98store Integration** vá»›i 9 premium models
- **ğŸ”§ Smart Tool Management** vá»›i provider compatibility
- Tool execution vÃ  response processing
- Error handling vÃ  fallback mechanisms

---

```mermaid
flowchart TD
    %% User starts conversation
    START(["ğŸ‘¤ User starts chat"]) --> UI

    %% Frontend Layer
    subgraph FE ["ğŸŒ Frontend Layer"]
        UI["ğŸ“± User Interface<br/>ThreadComponent.tsx"]
        CI["âœï¸ Chat Input<br/>chat-input.tsx"]
        MS["âš™ï¸ Model Selection<br/>use-model-selection.ts"]
        UI --> CI
        CI --> MS
    end

    %% API Gateway
    subgraph API ["ğŸ”— FastAPI Backend"]
        MAIN["âš¡ Main API<br/>api.py"]
        CORE["ğŸ¯ Core Router<br/>core/api_main.py"]
        RUNS["ğŸƒ Agent Runs<br/>core/agent_runs.py"]
        MAIN --> CORE
        CORE --> RUNS
    end

    %% Background Processing
    subgraph BG ["âš™ï¸ Background Processing"]
        DRAMA["ğŸ­ Dramatiq Worker<br/>run_agent_background.py"]
        REDIS["ğŸ“¦ Redis State<br/>Locks & Pub/Sub"]
        CORE_RUN["ğŸ§  Core Agent<br/>core/run.py"]
        DRAMA <--> REDIS
        DRAMA --> CORE_RUN
    end

    %% Model Management with Auto Detection
    AUTO{"ğŸ¤” Auto Model?<br/>model == 'auto'"}
    HEURISTIC["ğŸ¯ Smart Analysis<br/>Query complexity<br/>User tier"]
    RESOLVE["âš™ï¸ Model Resolution<br/>model_manager"]

    %% LLM Service Layer
    subgraph LLM ["ğŸ§  LLM Service"]
        PREP["ğŸ”§ Prepare Params<br/>prepare_params()"]
        ROUTER["ğŸ”„ LiteLLM Router<br/>acompletion()"]
        PROVIDERS["ğŸ¢ Providers<br/>OpenAI | Anthropic<br/>xAI | Groq"]
        PREP --> ROUTER
        ROUTER --> PROVIDERS
    end

    %% Tool System
    subgraph TOOLS ["ğŸ”§ Tool System"]
        TM["ğŸ› ï¸ Tool Manager"]
        TR["ğŸ“‹ Tool Registry"]
        EXEC["âš¡ Tool Execution"]
        TM --> TR
        TR --> EXEC
    end

    %% Response Processing
    STREAM{"ğŸŒŠ Streaming?"}
    PROC_S["ğŸ“¡ Stream Processor"]
    PROC_NS["ğŸ“„ Batch Processor"]
    PERSIST["ğŸ’¾ Database<br/>Supabase"]

    %% Error Handling
    subgraph ERROR ["ğŸš¨ Error Handling"]
        FALLBACK["ğŸ”„ Fallback<br/>OpenRouter"]
        RETRY["ğŸ” Retry Logic"]
        ERR_LOG["ğŸ“ Error Logs"]
        FALLBACK --> RETRY
        RETRY --> ERR_LOG
    end

    %% Context Management
    subgraph CONTEXT ["ğŸ§  Context Mgmt"]
        TOKEN["ğŸ”¢ Token Count"]
        COMPRESS["ğŸ“¦ Compression"]
        CACHE["ğŸ’¾ Caching"]
        TOKEN --> COMPRESS
        COMPRESS --> CACHE
    end

    %% Main Flow Connections
    MS --> MAIN
    RUNS --> DRAMA
    CORE_RUN --> AUTO
    AUTO -->|Yes| HEURISTIC
    AUTO -->|No| RESOLVE
    HEURISTIC --> RESOLVE
    RESOLVE --> PREP
    
    %% LLM to Tools
    PROVIDERS --> TOOLS
    EXEC --> STREAM
    
    %% Response Processing Flow
    STREAM -->|Yes| PROC_S
    STREAM -->|No| PROC_NS
    PROC_S --> PERSIST
    PROC_NS --> PERSIST
    
    %% Context Management Flow
    CORE_RUN --> CONTEXT
    CONTEXT --> PREP
    
    %% Error Handling Flow
    PROVIDERS -.->|"âŒ Error"| ERROR
    ERROR -.->|"ğŸ”„ Retry"| PROVIDERS
    
    %% Final Response
    PERSIST --> UI
    
    %% End point
    UI --> FINISH(["âœ… Response delivered"])

    %% Styling
    classDef frontend fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef backend fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef processing fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef llm fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef tools fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef decision fill:#fff9c4,stroke:#f9a825,stroke-width:3px
    classDef error fill:#ffebee,stroke:#d32f2f,stroke-width:2px
    classDef startend fill:#e0e0e0,stroke:#424242,stroke-width:2px

    class UI,CI,MS frontend
    class MAIN,CORE,RUNS backend
    class DRAMA,REDIS,CORE_RUN processing
    class PREP,ROUTER,PROVIDERS llm
    class TM,TR,EXEC tools
    class AUTO,HEURISTIC,STREAM decision
    class FALLBACK,RETRY,ERR_LOG error
    class START,FINISH startend
```

---

## Key Components Explanation

### ğŸŒ **Frontend Layer**
- **ThreadComponent.tsx**: Main chat interface quáº£n lÃ½ conversation state
- **chat-input.tsx**: Input component xá»­ lÃ½ user message submission
- **_use-model-selection.ts**: Hook quáº£n lÃ½ model selection vá»›i "auto" option

### ğŸ”— **FastAPI Backend**
- **api.py**: Main application vá»›i CORS, middleware setup
- **core/api_main.py**: Core routing vá»›i sub-routers cho various services
- **core/agent_runs.py**: Agent lifecycle management (start/stop/status)

### âš™ï¸ **Background Processing**
- **Dramatiq Worker**: Background task processing vá»›i Redis coordination
- **Redis State Management**: Locks, TTLs, pub/sub cho distributed coordination
- **Core Agent Run**: Main execution logic trong `core/run.py`

### ğŸ¤– **Smart Model Routing**
- **Auto Detection**: PhÃ¢n tÃ­ch query complexity Ä‘á»ƒ chá»n optimal model
- **Heuristics**: User tier consideration, cost optimization
- **Model Resolution**: `model_manager.resolve_model_id()` mapping

### ğŸ§  **LLM Service Layer**  
- **Parameter Preparation**: Provider-specific configurations
- **LiteLLM Router**: Unified interface cho multiple providers
- **Provider Support**: OpenAI, Anthropic, xAI, Groq, OpenRouter, Bedrock

### ğŸ”§ **Tool System**
- **Tool Manager**: Register various agent capabilities
- **Tool Registry**: OpenAPI schemas cho native tool calling
- **Tool Execution**: Function calls vá»›i error handling

### ğŸ“¤ **Response Processing**
- **Streaming Support**: Real-time response chunks
- **Non-Streaming**: Complete response processing
- **Database Persistence**: Supabase storage cho conversation history

### ğŸš¨ **Error Handling**
- **Fallback Mechanisms**: OpenRouter backup khi primary provider fails
- **Retry Logic**: Exponential backoff cho transient errors
- **Comprehensive Logging**: Structured logs vá»›i Langfuse tracing

### ğŸ§  **Context Management**
- **Token Counting**: Monitor context length limits
- **Message Compression**: Automatic summarization khi over threshold
- **Caching**: Anthropic cache control cho cost optimization

---

## Flow Descriptions

### ğŸ¯ **Main Request Flow**
1. User inputs message trong chat interface
2. Model selection (auto/specific) Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh
3. FastAPI routes request qua core API
4. Agent run Ä‘Æ°á»£c táº¡o vÃ  queued trong background
5. Dramatiq worker picks up task
6. Smart model routing analyzes vÃ  selects optimal model
7. LLM API call Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i appropriate parameters
8. Response Ä‘Æ°á»£c processed (streaming/non-streaming)
9. Tools Ä‘Æ°á»£c executed náº¿u needed
10. Results Ä‘Æ°á»£c persisted vÃ  returned to frontend

### ğŸ”„ **Auto Model Selection Flow**
1. Check if model === "auto"
2. Analyze query complexity vÃ  user tier
3. Apply heuristics Ä‘á»ƒ determine optimal model
4. Resolve model ID thÃ´ng qua model manager
5. Use resolved model cho LLM API call

### ğŸš¨ **Error Handling Flow**
1. Primary provider fails
2. Fallback to OpenRouter equivalent
3. Retry with exponential backoff
4. Log structured error information
5. Return user-friendly error message náº¿u all attempts fail

---

## Technical Notes

- **Redis Keys**: `active_run:{instance_id}:{agent_run_id}` format
- **Control Channels**: `agent_run:{agent_run_id}:control` cho stop signals
- **Response Lists**: `agent_run:{agent_run_id}:responses` cho streaming data
- **Tool Calling**: Native support thÃ´ng qua OpenAPI schemas
- **Context Limits**: Automatic compression khi token threshold exceeded
- **Provider Fallbacks**: Mapped cho high availability

---

*Diagram generated: 2025-01-18*  
*Source: ChainLens codebase analysis*