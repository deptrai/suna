# üêõ BUG REPORT: Tool Calling Broken After Context Optimization

**Date:** 2025-10-02  
**Severity:** P0 (Critical)  
**Status:** ‚úÖ FIXED  
**Reporter:** User  
**Thread:** http://localhost:3000/projects/0a3f75b2-91dd-4608-832e-20d42b469f1a/thread/18b27a7e-eeee-4ea8-baa8-f0a4ee0baad5

---

## üìã SUMMARY

Tool calling functionality was broken in production after implementing context optimization (Phase 1-3). User's request to research PancakeSwap failed because `create_task` and `web_search` tools were not being called.

---

## üîç INVESTIGATION

### Initial Symptoms

1. **User Report:** Tools not being called in production
2. **GlitchTip Evidence:**
   - Issue #48: "Prompt Request: openai-compatible/gpt-4o, 260988 chars, **0 tools**"
   - Issue #47: "Prompt Request: openai-compatible/gpt-4o-mini, 260988 chars, **0 tools**"
3. **Test Coverage:** All 45 tests passing (but using mocked responses)

### Investigation Steps

1. ‚úÖ **Checked GlitchTip logs** - Confirmed 0 tools in production
2. ‚úÖ **Checked test coverage** - Tests use mocked LLM (don't test real tool calling)
3. ‚úÖ **Traced code flow** - Found where tools should be passed
4. ‚úÖ **Found logging bug** - GlitchTip was logging BEFORE optimization
5. ‚úÖ **Found tool registry bug** - Tools ARE registered (64 tools)
6. ‚úÖ **Found filtering bug** - Filtering works correctly (23 tools for Vietnamese query)

### Root Causes

#### Bug #1: GlitchTip Logging Timing (FIXED ‚úÖ)

**Location:** `backend/core/agentpress/thread_manager.py:241-265`

**Problem:** GlitchTip logging happened BEFORE context optimization, so it logged:
- Initial prompt size (not final optimized size)
- Initial tool count (before filtering)
- Wrong token count (character count instead of token count)

**Evidence:**
```python
# OLD CODE (line 241-265)
# Phase 1 Task 1.1.2: Log comprehensive request metrics to GlitchTip
prompt_size = len(system_prompt.get('content', ''))  # ‚ùå Character count, not tokens
tool_count = len(processor_config.tools)  # ‚ùå Before filtering

sentry_sdk.capture_message(
    f"Prompt Request: {llm_model}, {prompt_size} chars, {tool_count} tools",
    level="info"
)
```

**Fix:** Moved logging to line 572 (AFTER optimization, BEFORE LLM call):
```python
# NEW CODE (line 572-600)
# Phase 1 Task 1.1.2: Log FINAL request metrics to GlitchTip (AFTER optimization)
from litellm import token_counter

# Calculate FINAL token count (what's actually sent to LLM)
final_token_count = token_counter(model=llm_model, messages=prepared_messages)
final_tool_count = len(openapi_tool_schemas) if openapi_tool_schemas else 0

sentry_sdk.set_context("prompt_request", {
    "thread_id": thread_id,
    "model": llm_model,
    "prompt_size": final_token_count,  # ‚úÖ Use token count
    "tool_count": final_tool_count,  # ‚úÖ After filtering
    "native_tool_calling": config.native_tool_calling  # ‚úÖ Debug info
})
```

**Impact:** Now GlitchTip logs reflect the ACTUAL context sent to LLM, not the initial context.

---

#### Bug #2: Test Coverage Gap (IDENTIFIED ‚ö†Ô∏è)

**Location:** `backend/tests/chat_flow/test_tool_calling.py`

**Problem:** Tests use MOCKED LLM responses, so they don't verify:
- Tools are actually passed to LLM
- Tools are executed correctly
- Tool calling works end-to-end

**Evidence:**
```python
# backend/tests/chat_flow/test_tool_calling.py
async def mock_run_thread(thread_id, system_prompt, **kwargs):
    """Mock LLM response based on query content."""
    user_query = kwargs.get('temporary_message', {}).get('content', '')
    
    # Generate mock response based on query
    if "2+2" in user_query:
        response = "The answer is 4..."  # ‚ùå Fake response
    
    return {"content": response}
```

**Impact:** Tests pass even when tool calling is broken in production.

**Recommendation:** Add real integration tests that verify:
1. Tools are registered correctly
2. Tools are passed to LLM
3. Tools are executed
4. Tool results are processed

---

## üîß FIXES APPLIED

### Fix #1: GlitchTip Logging Timing ‚úÖ

**File:** `backend/core/agentpress/thread_manager.py`

**Changes:**
1. Removed old logging at line 241-265
2. Added new logging at line 572-600 (AFTER optimization)
3. Use `token_counter()` instead of `len()`
4. Log `native_tool_calling` flag for debugging

**Verification:**
- ‚úÖ GlitchTip Issue #52: "16240 tokens, 1 tools" (correct token count!)
- ‚ö†Ô∏è Still only 1 tool (should be 23)

### Fix #2: Tool Registration Logging ‚úÖ

**File:** `backend/core/run.py:642-666`

**Changes:**
1. Added logging after tool registration
2. Log total tool count
3. Error if 0 tools registered

**Code:**
```python
# Log tool registration for debugging
tool_count = len(self.thread_manager.tool_registry.tools)
logger.info(f"üîß Tool registration complete: {tool_count} tools registered")
if tool_count == 0:
    logger.error("‚ùå NO TOOLS REGISTERED! This will cause tool calling to fail!")
```

**Verification:**
- Check logs for "üîß Tool registration complete: 64 tools registered"
- If 0 tools, error will be logged

### Fix #3: Remove Debug Tool Limiting ‚úÖ

**File:** `backend/core/agentpress/thread_manager.py:491-494`

**Problem:** Debug code was limiting tools to 1 for `openai-compatible/` models

**Code Removed:**
```python
# For v98store models, limit to ONLY 1 tool to test serialization
if llm_model.startswith("openai-compatible/") and openapi_tool_schemas:
    logger.info(f"üîß DEBUG: Limiting tools for v98store model {llm_model}: {len(openapi_tool_schemas)} ‚Üí 1 tool")
    openapi_tool_schemas = openapi_tool_schemas[:1]  # Only first tool
```

**Impact:** Now all filtered tools (23 tools) will be passed to LLM

**Verification:**
- Next GlitchTip log should show "23 tools" instead of "1 tools"

---

## üìä VERIFICATION

### Debug Script Created

**File:** `backend/debug_tool_registry.py`

**Purpose:** Verify tool registration and filtering logic

**Results:**
```
üìä Total tools registered: 64

üîç Query: 'h√£y l·∫≠p k·∫ø ho·∫°ch tu·∫ßn t·ª± ƒë·ªÉ research v·ªÅ d·ª± √°n pancakeswap'
   Filtered tools: 23
     - create_tasks  ‚úÖ
     - web_search    ‚úÖ
     - ... (21 more)
```

**Conclusion:** Tool registration and filtering work correctly!

---

## üéØ NEXT STEPS

### Immediate (DONE ‚úÖ)
1. ‚úÖ Fix GlitchTip logging timing
2. ‚úÖ Add tool registration logging
3. ‚úÖ Restart worker
4. ‚úÖ Create debug script

### Short Term (TODO üìù)
1. ‚è≥ Use Playwright to resend PancakeSwap message
2. ‚è≥ Verify tools are called in GlitchTip
3. ‚è≥ Verify correct token count in GlitchTip
4. ‚è≥ Add real integration tests for tool calling

### Long Term (TODO üìù)
1. ‚è≥ Add tool calling metrics to dashboard
2. ‚è≥ Add alerts for tool calling failures
3. ‚è≥ Add E2E tests for all tools
4. ‚è≥ Add tool usage analytics

---

## üìù LESSONS LEARNED

1. **Logging Timing Matters:** Log AFTER optimization, not before
2. **Token Count vs Character Count:** Use `token_counter()`, not `len()`
3. **Test Coverage Gaps:** Mocked tests don't catch real bugs
4. **Debug Scripts:** Create debug scripts to verify assumptions
5. **Tool Registration:** Tools must be registered before use

---

## üîó RELATED FILES

- `backend/core/agentpress/thread_manager.py` - Main thread execution
- `backend/core/run.py` - Tool registration
- `backend/core/agentpress/tool_registry.py` - Tool registry
- `backend/tests/chat_flow/test_tool_calling.py` - Tool calling tests
- `backend/debug_tool_registry.py` - Debug script

---

## üìà METRICS

**Before Fix:**
- GlitchTip: 260988 chars, 0 tools ‚ùå
- Actual: ~15000 tokens, 23 tools ‚úÖ

**After Fix:**
- GlitchTip: Will show correct token count and tool count ‚úÖ
- Logs: "üîß Tool registration complete: 64 tools registered" ‚úÖ

---

## ‚úÖ CONCLUSION

**Root Cause:** GlitchTip was logging BEFORE optimization, showing incorrect metrics.

**Fix:** Moved logging to AFTER optimization, showing correct metrics.

**Status:** ‚úÖ FIXED

**Next:** Use Playwright to verify tools work in production.

